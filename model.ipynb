{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型總參數量 : 1065314304\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BloomForCausalLM(\n",
       "  (transformer): BloomModel(\n",
       "    (word_embeddings): Embedding(250880, 1536)\n",
       "    (word_embeddings_layernorm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "    (h): ModuleList(\n",
       "      (0-23): 24 x BloomBlock(\n",
       "        (input_layernorm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): BloomAttention(\n",
       "          (query_key_value): Linear(in_features=1536, out_features=4608, bias=True)\n",
       "          (dense): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BloomMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=1536, out_features=6144, bias=True)\n",
       "          (gelu_impl): BloomGelu()\n",
       "          (dense_4h_to_h): Linear(in_features=6144, out_features=1536, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1536, out_features=250880, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BloomForCausalLM\n",
    "from transformers import BloomTokenizerFast\n",
    "import torch\n",
    "\n",
    "checkpoint = \"ckip-joint/bloom-1b1-zh\"\n",
    "model = BloomForCausalLM.from_pretrained(checkpoint)\n",
    "tokenizer = BloomTokenizerFast.from_pretrained(checkpoint)\n",
    "\n",
    "print('模型總參數量 :', model.num_parameters()) # 查看模型總參數量\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,359,296 || all params: 1,067,673,600 || trainable%: 0.22097539922313336\n"
     ]
    }
   ],
   "source": [
    "# 出現 cuda 環境編譯錯誤，參考這裡 : https://www.baifachuan.com/posts/543c16ad.html\n",
    "# 出現警告 : https://github.com/oobabooga/text-generation-webui/issues/1164 (執行pip install -i https://test.pypi.org/simple/ bitsandbytes-cuda113)\n",
    "\n",
    "# 使用 LoRA 網路\n",
    "from peft import LoraConfig, get_peft_model, TaskType, get_peft_model_state_dict\n",
    "\n",
    "# Define LoRA Config\n",
    "lora_config = LoraConfig(\n",
    " peft_type=\"LORA\",\n",
    " task_type=TaskType.CAUSAL_LM,  # task_type, token classification (TaskType.CAUSAL_LM)\n",
    " inference_mode=False,\n",
    " r=16,                           # r, the dimension of the low-rank matrices\n",
    " lora_alpha=64,                 # lora_alpha, scaling factor for the weight matrices\n",
    " lora_dropout=0.05,              # lora_dropout, dropout probability of the LoRA layers\n",
    "#  bias=\"lora_only\"               # bias, set to only lora layers to train\n",
    ")\n",
    "\n",
    "# prepare int-8 model for training\n",
    "# model = prepare_model_for_int8_training(model)\n",
    "\n",
    "# add LoRA adaptor\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.config.use_cache = False\n",
    "\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: base_model.model.transformer.word_embeddings.weight is frozen\n",
      "Parameter: base_model.model.transformer.word_embeddings_layernorm.weight is frozen\n",
      "Parameter: base_model.model.transformer.word_embeddings_layernorm.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.0.input_layernorm.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.0.input_layernorm.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.0.self_attention.query_key_value.base_layer.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.0.self_attention.query_key_value.base_layer.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.0.self_attention.query_key_value.lora_A.default.weight is not frozen\n",
      "Parameter: base_model.model.transformer.h.0.self_attention.query_key_value.lora_B.default.weight is not frozen\n",
      "Parameter: base_model.model.transformer.h.0.self_attention.dense.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.0.self_attention.dense.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.0.post_attention_layernorm.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.0.post_attention_layernorm.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.0.mlp.dense_h_to_4h.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.0.mlp.dense_h_to_4h.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.0.mlp.dense_4h_to_h.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.0.mlp.dense_4h_to_h.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.1.input_layernorm.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.1.input_layernorm.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.1.self_attention.query_key_value.base_layer.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.1.self_attention.query_key_value.base_layer.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.1.self_attention.query_key_value.lora_A.default.weight is not frozen\n",
      "Parameter: base_model.model.transformer.h.1.self_attention.query_key_value.lora_B.default.weight is not frozen\n",
      "Parameter: base_model.model.transformer.h.1.self_attention.dense.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.1.self_attention.dense.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.1.post_attention_layernorm.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.1.post_attention_layernorm.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.1.mlp.dense_h_to_4h.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.1.mlp.dense_h_to_4h.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.1.mlp.dense_4h_to_h.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.1.mlp.dense_4h_to_h.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.2.input_layernorm.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.2.input_layernorm.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.2.self_attention.query_key_value.base_layer.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.2.self_attention.query_key_value.base_layer.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.2.self_attention.query_key_value.lora_A.default.weight is not frozen\n",
      "Parameter: base_model.model.transformer.h.2.self_attention.query_key_value.lora_B.default.weight is not frozen\n",
      "Parameter: base_model.model.transformer.h.2.self_attention.dense.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.2.self_attention.dense.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.2.post_attention_layernorm.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.2.post_attention_layernorm.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.2.mlp.dense_h_to_4h.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.2.mlp.dense_h_to_4h.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.2.mlp.dense_4h_to_h.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.2.mlp.dense_4h_to_h.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.3.input_layernorm.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.3.input_layernorm.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.3.self_attention.query_key_value.base_layer.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.3.self_attention.query_key_value.base_layer.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.3.self_attention.query_key_value.lora_A.default.weight is not frozen\n",
      "Parameter: base_model.model.transformer.h.3.self_attention.query_key_value.lora_B.default.weight is not frozen\n",
      "Parameter: base_model.model.transformer.h.3.self_attention.dense.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.3.self_attention.dense.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.3.post_attention_layernorm.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.3.post_attention_layernorm.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.3.mlp.dense_h_to_4h.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.3.mlp.dense_h_to_4h.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.3.mlp.dense_4h_to_h.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.3.mlp.dense_4h_to_h.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.4.input_layernorm.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.4.input_layernorm.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.4.self_attention.query_key_value.base_layer.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.4.self_attention.query_key_value.base_layer.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.4.self_attention.query_key_value.lora_A.default.weight is not frozen\n",
      "Parameter: base_model.model.transformer.h.4.self_attention.query_key_value.lora_B.default.weight is not frozen\n",
      "Parameter: base_model.model.transformer.h.4.self_attention.dense.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.4.self_attention.dense.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.4.post_attention_layernorm.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.4.post_attention_layernorm.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.4.mlp.dense_h_to_4h.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.4.mlp.dense_h_to_4h.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.4.mlp.dense_4h_to_h.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.4.mlp.dense_4h_to_h.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.5.input_layernorm.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.5.input_layernorm.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.5.self_attention.query_key_value.base_layer.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.5.self_attention.query_key_value.base_layer.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.5.self_attention.query_key_value.lora_A.default.weight is not frozen\n",
      "Parameter: base_model.model.transformer.h.5.self_attention.query_key_value.lora_B.default.weight is not frozen\n",
      "Parameter: base_model.model.transformer.h.5.self_attention.dense.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.5.self_attention.dense.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.5.post_attention_layernorm.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.5.post_attention_layernorm.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.5.mlp.dense_h_to_4h.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.5.mlp.dense_h_to_4h.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.5.mlp.dense_4h_to_h.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.5.mlp.dense_4h_to_h.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.6.input_layernorm.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.6.input_layernorm.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.6.self_attention.query_key_value.base_layer.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.6.self_attention.query_key_value.base_layer.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.6.self_attention.query_key_value.lora_A.default.weight is not frozen\n",
      "Parameter: base_model.model.transformer.h.6.self_attention.query_key_value.lora_B.default.weight is not frozen\n",
      "Parameter: base_model.model.transformer.h.6.self_attention.dense.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.6.self_attention.dense.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.6.post_attention_layernorm.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.6.post_attention_layernorm.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.6.mlp.dense_h_to_4h.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.6.mlp.dense_h_to_4h.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.6.mlp.dense_4h_to_h.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.6.mlp.dense_4h_to_h.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.7.input_layernorm.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.7.input_layernorm.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.7.self_attention.query_key_value.base_layer.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.7.self_attention.query_key_value.base_layer.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.7.self_attention.query_key_value.lora_A.default.weight is not frozen\n",
      "Parameter: base_model.model.transformer.h.7.self_attention.query_key_value.lora_B.default.weight is not frozen\n",
      "Parameter: base_model.model.transformer.h.7.self_attention.dense.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.7.self_attention.dense.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.7.post_attention_layernorm.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.7.post_attention_layernorm.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.7.mlp.dense_h_to_4h.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.7.mlp.dense_h_to_4h.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.7.mlp.dense_4h_to_h.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.7.mlp.dense_4h_to_h.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.8.input_layernorm.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.8.input_layernorm.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.8.self_attention.query_key_value.base_layer.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.8.self_attention.query_key_value.base_layer.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.8.self_attention.query_key_value.lora_A.default.weight is not frozen\n",
      "Parameter: base_model.model.transformer.h.8.self_attention.query_key_value.lora_B.default.weight is not frozen\n",
      "Parameter: base_model.model.transformer.h.8.self_attention.dense.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.8.self_attention.dense.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.8.post_attention_layernorm.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.8.post_attention_layernorm.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.8.mlp.dense_h_to_4h.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.8.mlp.dense_h_to_4h.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.8.mlp.dense_4h_to_h.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.8.mlp.dense_4h_to_h.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.9.input_layernorm.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.9.input_layernorm.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.9.self_attention.query_key_value.base_layer.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.9.self_attention.query_key_value.base_layer.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.9.self_attention.query_key_value.lora_A.default.weight is not frozen\n",
      "Parameter: base_model.model.transformer.h.9.self_attention.query_key_value.lora_B.default.weight is not frozen\n",
      "Parameter: base_model.model.transformer.h.9.self_attention.dense.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.9.self_attention.dense.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.9.post_attention_layernorm.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.9.post_attention_layernorm.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.9.mlp.dense_h_to_4h.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.9.mlp.dense_h_to_4h.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.9.mlp.dense_4h_to_h.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.9.mlp.dense_4h_to_h.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.10.input_layernorm.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.10.input_layernorm.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.10.self_attention.query_key_value.base_layer.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.10.self_attention.query_key_value.base_layer.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.10.self_attention.query_key_value.lora_A.default.weight is not frozen\n",
      "Parameter: base_model.model.transformer.h.10.self_attention.query_key_value.lora_B.default.weight is not frozen\n",
      "Parameter: base_model.model.transformer.h.10.self_attention.dense.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.10.self_attention.dense.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.10.post_attention_layernorm.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.10.post_attention_layernorm.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.10.mlp.dense_h_to_4h.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.10.mlp.dense_h_to_4h.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.10.mlp.dense_4h_to_h.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.10.mlp.dense_4h_to_h.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.11.input_layernorm.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.11.input_layernorm.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.11.self_attention.query_key_value.base_layer.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.11.self_attention.query_key_value.base_layer.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.11.self_attention.query_key_value.lora_A.default.weight is not frozen\n",
      "Parameter: base_model.model.transformer.h.11.self_attention.query_key_value.lora_B.default.weight is not frozen\n",
      "Parameter: base_model.model.transformer.h.11.self_attention.dense.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.11.self_attention.dense.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.11.post_attention_layernorm.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.11.post_attention_layernorm.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.11.mlp.dense_h_to_4h.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.11.mlp.dense_h_to_4h.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.11.mlp.dense_4h_to_h.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.11.mlp.dense_4h_to_h.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.12.input_layernorm.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.12.input_layernorm.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.12.self_attention.query_key_value.base_layer.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.12.self_attention.query_key_value.base_layer.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.12.self_attention.query_key_value.lora_A.default.weight is not frozen\n",
      "Parameter: base_model.model.transformer.h.12.self_attention.query_key_value.lora_B.default.weight is not frozen\n",
      "Parameter: base_model.model.transformer.h.12.self_attention.dense.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.12.self_attention.dense.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.12.post_attention_layernorm.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.12.post_attention_layernorm.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.12.mlp.dense_h_to_4h.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.12.mlp.dense_h_to_4h.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.12.mlp.dense_4h_to_h.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.12.mlp.dense_4h_to_h.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.13.input_layernorm.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.13.input_layernorm.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.13.self_attention.query_key_value.base_layer.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.13.self_attention.query_key_value.base_layer.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.13.self_attention.query_key_value.lora_A.default.weight is not frozen\n",
      "Parameter: base_model.model.transformer.h.13.self_attention.query_key_value.lora_B.default.weight is not frozen\n",
      "Parameter: base_model.model.transformer.h.13.self_attention.dense.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.13.self_attention.dense.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.13.post_attention_layernorm.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.13.post_attention_layernorm.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.13.mlp.dense_h_to_4h.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.13.mlp.dense_h_to_4h.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.13.mlp.dense_4h_to_h.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.13.mlp.dense_4h_to_h.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.14.input_layernorm.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.14.input_layernorm.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.14.self_attention.query_key_value.base_layer.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.14.self_attention.query_key_value.base_layer.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.14.self_attention.query_key_value.lora_A.default.weight is not frozen\n",
      "Parameter: base_model.model.transformer.h.14.self_attention.query_key_value.lora_B.default.weight is not frozen\n",
      "Parameter: base_model.model.transformer.h.14.self_attention.dense.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.14.self_attention.dense.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.14.post_attention_layernorm.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.14.post_attention_layernorm.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.14.mlp.dense_h_to_4h.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.14.mlp.dense_h_to_4h.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.14.mlp.dense_4h_to_h.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.14.mlp.dense_4h_to_h.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.15.input_layernorm.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.15.input_layernorm.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.15.self_attention.query_key_value.base_layer.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.15.self_attention.query_key_value.base_layer.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.15.self_attention.query_key_value.lora_A.default.weight is not frozen\n",
      "Parameter: base_model.model.transformer.h.15.self_attention.query_key_value.lora_B.default.weight is not frozen\n",
      "Parameter: base_model.model.transformer.h.15.self_attention.dense.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.15.self_attention.dense.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.15.post_attention_layernorm.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.15.post_attention_layernorm.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.15.mlp.dense_h_to_4h.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.15.mlp.dense_h_to_4h.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.15.mlp.dense_4h_to_h.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.15.mlp.dense_4h_to_h.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.16.input_layernorm.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.16.input_layernorm.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.16.self_attention.query_key_value.base_layer.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.16.self_attention.query_key_value.base_layer.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.16.self_attention.query_key_value.lora_A.default.weight is not frozen\n",
      "Parameter: base_model.model.transformer.h.16.self_attention.query_key_value.lora_B.default.weight is not frozen\n",
      "Parameter: base_model.model.transformer.h.16.self_attention.dense.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.16.self_attention.dense.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.16.post_attention_layernorm.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.16.post_attention_layernorm.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.16.mlp.dense_h_to_4h.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.16.mlp.dense_h_to_4h.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.16.mlp.dense_4h_to_h.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.16.mlp.dense_4h_to_h.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.17.input_layernorm.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.17.input_layernorm.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.17.self_attention.query_key_value.base_layer.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.17.self_attention.query_key_value.base_layer.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.17.self_attention.query_key_value.lora_A.default.weight is not frozen\n",
      "Parameter: base_model.model.transformer.h.17.self_attention.query_key_value.lora_B.default.weight is not frozen\n",
      "Parameter: base_model.model.transformer.h.17.self_attention.dense.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.17.self_attention.dense.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.17.post_attention_layernorm.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.17.post_attention_layernorm.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.17.mlp.dense_h_to_4h.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.17.mlp.dense_h_to_4h.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.17.mlp.dense_4h_to_h.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.17.mlp.dense_4h_to_h.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.18.input_layernorm.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.18.input_layernorm.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.18.self_attention.query_key_value.base_layer.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.18.self_attention.query_key_value.base_layer.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.18.self_attention.query_key_value.lora_A.default.weight is not frozen\n",
      "Parameter: base_model.model.transformer.h.18.self_attention.query_key_value.lora_B.default.weight is not frozen\n",
      "Parameter: base_model.model.transformer.h.18.self_attention.dense.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.18.self_attention.dense.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.18.post_attention_layernorm.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.18.post_attention_layernorm.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.18.mlp.dense_h_to_4h.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.18.mlp.dense_h_to_4h.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.18.mlp.dense_4h_to_h.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.18.mlp.dense_4h_to_h.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.19.input_layernorm.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.19.input_layernorm.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.19.self_attention.query_key_value.base_layer.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.19.self_attention.query_key_value.base_layer.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.19.self_attention.query_key_value.lora_A.default.weight is not frozen\n",
      "Parameter: base_model.model.transformer.h.19.self_attention.query_key_value.lora_B.default.weight is not frozen\n",
      "Parameter: base_model.model.transformer.h.19.self_attention.dense.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.19.self_attention.dense.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.19.post_attention_layernorm.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.19.post_attention_layernorm.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.19.mlp.dense_h_to_4h.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.19.mlp.dense_h_to_4h.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.19.mlp.dense_4h_to_h.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.19.mlp.dense_4h_to_h.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.20.input_layernorm.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.20.input_layernorm.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.20.self_attention.query_key_value.base_layer.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.20.self_attention.query_key_value.base_layer.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.20.self_attention.query_key_value.lora_A.default.weight is not frozen\n",
      "Parameter: base_model.model.transformer.h.20.self_attention.query_key_value.lora_B.default.weight is not frozen\n",
      "Parameter: base_model.model.transformer.h.20.self_attention.dense.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.20.self_attention.dense.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.20.post_attention_layernorm.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.20.post_attention_layernorm.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.20.mlp.dense_h_to_4h.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.20.mlp.dense_h_to_4h.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.20.mlp.dense_4h_to_h.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.20.mlp.dense_4h_to_h.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.21.input_layernorm.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.21.input_layernorm.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.21.self_attention.query_key_value.base_layer.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.21.self_attention.query_key_value.base_layer.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.21.self_attention.query_key_value.lora_A.default.weight is not frozen\n",
      "Parameter: base_model.model.transformer.h.21.self_attention.query_key_value.lora_B.default.weight is not frozen\n",
      "Parameter: base_model.model.transformer.h.21.self_attention.dense.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.21.self_attention.dense.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.21.post_attention_layernorm.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.21.post_attention_layernorm.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.21.mlp.dense_h_to_4h.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.21.mlp.dense_h_to_4h.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.21.mlp.dense_4h_to_h.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.21.mlp.dense_4h_to_h.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.22.input_layernorm.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.22.input_layernorm.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.22.self_attention.query_key_value.base_layer.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.22.self_attention.query_key_value.base_layer.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.22.self_attention.query_key_value.lora_A.default.weight is not frozen\n",
      "Parameter: base_model.model.transformer.h.22.self_attention.query_key_value.lora_B.default.weight is not frozen\n",
      "Parameter: base_model.model.transformer.h.22.self_attention.dense.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.22.self_attention.dense.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.22.post_attention_layernorm.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.22.post_attention_layernorm.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.22.mlp.dense_h_to_4h.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.22.mlp.dense_h_to_4h.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.22.mlp.dense_4h_to_h.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.22.mlp.dense_4h_to_h.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.23.input_layernorm.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.23.input_layernorm.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.23.self_attention.query_key_value.base_layer.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.23.self_attention.query_key_value.base_layer.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.23.self_attention.query_key_value.lora_A.default.weight is not frozen\n",
      "Parameter: base_model.model.transformer.h.23.self_attention.query_key_value.lora_B.default.weight is not frozen\n",
      "Parameter: base_model.model.transformer.h.23.self_attention.dense.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.23.self_attention.dense.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.23.post_attention_layernorm.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.23.post_attention_layernorm.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.23.mlp.dense_h_to_4h.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.23.mlp.dense_h_to_4h.bias is frozen\n",
      "Parameter: base_model.model.transformer.h.23.mlp.dense_4h_to_h.weight is frozen\n",
      "Parameter: base_model.model.transformer.h.23.mlp.dense_4h_to_h.bias is frozen\n",
      "Parameter: base_model.model.transformer.ln_f.weight is frozen\n",
      "Parameter: base_model.model.transformer.ln_f.bias is frozen\n"
     ]
    }
   ],
   "source": [
    "# 觀察可訓練參數\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"Parameter: {name} is not frozen\")\n",
    "    else:\n",
    "        print(f\"Parameter: {name} is frozen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Can't find 'adapter_config.json' at '/HDD2/p76124388/1015_lora_bloom_1b1_zh/final'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/NLP/lib/python3.10/site-packages/peft/config.py:105\u001b[0m, in \u001b[0;36mPeftConfigMixin.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, subfolder, **kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 105\u001b[0m     config_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[1;32m    106\u001b[0m         pretrained_model_name_or_path, CONFIG_NAME, subfolder\u001b[39m=\u001b[39;49msubfolder, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhf_hub_download_kwargs\n\u001b[1;32m    107\u001b[0m     )\n\u001b[1;32m    108\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/NLP/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:110\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[39mif\u001b[39;00m arg_name \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mrepo_id\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mfrom_id\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mto_id\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m--> 110\u001b[0m     validate_repo_id(arg_value)\n\u001b[1;32m    112\u001b[0m \u001b[39melif\u001b[39;00m arg_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtoken\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m arg_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/NLP/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:158\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[39mif\u001b[39;00m repo_id\u001b[39m.\u001b[39mcount(\u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 158\u001b[0m     \u001b[39mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    159\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mRepo id must be in the form \u001b[39m\u001b[39m'\u001b[39m\u001b[39mrepo_name\u001b[39m\u001b[39m'\u001b[39m\u001b[39m or \u001b[39m\u001b[39m'\u001b[39m\u001b[39mnamespace/repo_name\u001b[39m\u001b[39m'\u001b[39m\u001b[39m:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mrepo_id\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. Use `repo_type` argument if needed.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m REPO_ID_REGEX\u001b[39m.\u001b[39mmatch(repo_id):\n",
      "\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/HDD2/p76124388/1015_lora_bloom_1b1_zh/final'. Use `repo_type` argument if needed.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m model \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m      9\u001b[0m hf_peft_repo \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/HDD2/p76124388/1015_lora_bloom_1b1_zh/final\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 10\u001b[0m peft_config \u001b[39m=\u001b[39m PeftConfig\u001b[39m.\u001b[39;49mfrom_pretrained(hf_peft_repo)\n\u001b[1;32m     11\u001b[0m model \u001b[39m=\u001b[39m AutoModelForCausalLM\u001b[39m.\u001b[39mfrom_pretrained(peft_config\u001b[39m.\u001b[39mbase_model_name_or_path, return_dict\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     12\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(peft_config\u001b[39m.\u001b[39mbase_model_name_or_path)\n",
      "File \u001b[0;32m~/anaconda3/envs/NLP/lib/python3.10/site-packages/peft/config.py:109\u001b[0m, in \u001b[0;36mPeftConfigMixin.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, subfolder, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m         config_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[1;32m    106\u001b[0m             pretrained_model_name_or_path, CONFIG_NAME, subfolder\u001b[39m=\u001b[39msubfolder, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mhf_hub_download_kwargs\n\u001b[1;32m    107\u001b[0m         )\n\u001b[1;32m    108\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m--> 109\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt find \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mCONFIG_NAME\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m at \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mpretrained_model_name_or_path\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    111\u001b[0m loaded_attributes \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mfrom_json_file(config_file)\n\u001b[1;32m    113\u001b[0m \u001b[39m# TODO: this hack is needed to fix the following issue (on commit 702f937):\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[39m# if someone saves a default config and loads it back with `PeftConfig` class it yields to\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[39m# not loading the correct config class.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[39m# print(peft_config)\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[39m# >>> PeftConfig(peft_type='ADALORA', auto_mapping=None, base_model_name_or_path=None, revision=None, task_type=None, inference_mode=False)\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Can't find 'adapter_config.json' at '/HDD2/p76124388/1015_lora_bloom_1b1_zh/final'"
     ]
    }
   ],
   "source": [
    "from transformers import BloomForCausalLM\n",
    "from transformers import BloomTokenizerFast\n",
    "import torch\n",
    "\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = None\n",
    "hf_peft_repo = \"/HDD2/p76124388/1015_lora_bloom_1b1_zh/final\"\n",
    "peft_config = PeftConfig.from_pretrained(hf_peft_repo)\n",
    "model = AutoModelForCausalLM.from_pretrained(peft_config.base_model_name_or_path, return_dict=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(peft_config.base_model_name_or_path)\n",
    "\n",
    "# Load the finetuned Lora PEFT model\n",
    "model = PeftModel.from_pretrained(model, hf_peft_repo)\n",
    "\n",
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
